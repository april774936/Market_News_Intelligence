import os, json, urllib.parse, feedparser, time
from datetime import datetime
from googleapiclient.discovery import build
from googleapiclient.http import MediaInMemoryUpload
from oauth2client.service_account import ServiceAccountCredentials

def run_mega_backfill():
    print("ğŸ¬ í”„ë¡œì íŠ¸ ì‹œì‘: ì¸ì¦ ì ˆì°¨ ì§„í–‰ ì¤‘...")
    scope = ["https://www.googleapis.com/auth/drive.file"]
    
    try:
        creds_json = json.loads(os.environ.get('GSPREAD_JSON'))
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_json, scope)
        drive_service = build('drive', 'v3', credentials=creds)
        print("ğŸ”‘ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì¸ì¦ ì„±ê³µ")
    except Exception as e:
        print(f"ğŸš¨ ì¸ì¦ ì‹¤íŒ¨: {e}")
        return

    BACKFILL_FOLDER_ID = "1-aITCmfSiRZ1eNLnqvt071PyyqA9DjbT"

    # ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë‹¨ìˆœí™”í•˜ì—¬ ì„±ê³µë¥ ì„ ë†’ì…ë‹ˆë‹¤.
    search_queries = [
        "Nasdaq", "S&P 500", "Federal Reserve", "Nvidia", "Tesla", 
        "Apple", "Inflation", "Interest Rate", "Bitcoin", "Gold"
    ]
    
    all_headlines = []

    print(f"ğŸš€ ì´ {len(search_queries)}ê°œ í…Œë§ˆ ìˆ˜ì§‘ ì‹œì‘...")
    
    for q in search_queries:
        try:
            # ì£¼ì†Œ êµ¬ì„±ì„ ë” ë‹¨ìˆœí•˜ê²Œ ë³€ê²½ (when:30d -> when:7d ë¡œ ì•ˆì •ì„± í™•ë³´)
            encoded_query = urllib.parse.quote(q)
            url = f"https://news.google.com/rss/search?q={encoded_query}+when:7d&hl=en-US&gl=US&ceid=US:en"
            
            print(f"ğŸ“¡ ìš”ì²­ ì¤‘: {q}...", end=" ")
            feed = feedparser.parse(url)
            
            if not feed.entries:
                print("âŒ ê²°ê³¼ ì—†ìŒ")
                continue
                
            count = len(feed.entries)
            print(f"âœ… {count}ê°œ ë°œê²¬")
            
            for entry in feed.entries:
                # ë°ì´í„° ì •ê·œí™”: [ë‚ ì§œ] | [í‚¤ì›Œë“œ] | [ì œëª©]
                clean_title = entry.title.replace('|', '-') # êµ¬ë¶„ì ì¤‘ë³µ ë°©ì§€
                all_headlines.append(f"{entry.published} | {q} | {clean_title}")
            
            time.sleep(1) # ì°¨ë‹¨ ë°©ì§€
        except Exception as e:
            print(f"âš ï¸ {q} ì—ëŸ¬: {e}")

    if not all_headlines:
        print("ğŸš¨ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ìµœì¢…ì ìœ¼ë¡œ 0ê±´ì…ë‹ˆë‹¤. RSS ì ‘ì† í™˜ê²½ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.")
        return

    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    all_headlines = list(set(all_headlines))
    all_headlines.sort()
    print(f"ğŸ”¥ ì´ {len(all_headlines)}ê°œì˜ ê³ ìœ  í—¤ë“œë¼ì¸ í™•ë³´!")

    # ë°ì´í„° ì €ì¥
    chunk_size = 150 
    for i in range(0, len(all_headlines), chunk_size):
        chunk = all_headlines[i:i + chunk_size]
        file_content = "DATE | CATEGORY | HEADLINE\n" + "="*60 + "\n"
        file_content += "\n".join(chunk)
        
        file_name = f"MEGA_Archive_Part_{ (i//chunk_size)+1 :02d}.txt"
        file_metadata = {'name': file_name, 'parents': [BACKFILL_FOLDER_ID]}
        media = MediaInMemoryUpload(file_content.encode('utf-8'), mimetype='text/plain')
        
        try:
            drive_service.files().create(body=file_metadata, media_body=media).execute()
            print(f"ğŸ“¤ {file_name} ì—…ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ {file_name} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    run_mega_backfill()
