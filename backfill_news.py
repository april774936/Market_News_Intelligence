import os, json, urllib.parse, feedparser, time
from datetime import datetime
from googleapiclient.discovery import build
from googleapiclient.http import MediaInMemoryUpload
from oauth2client.service_account import ServiceAccountCredentials

def main():
    print("--- ğŸš€ MEGA ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ê°€ë™ ì‹œì‘ ---")
    
    # 1. ë“œë¼ì´ë¸Œ ì¸ì¦
    try:
        scope = ["https://www.googleapis.com/auth/drive.file"]
        # GitHub Secretsì— ì €ì¥ëœ JSON í‚¤ ë¡œë“œ
        creds_json = json.loads(os.environ.get('GSPREAD_JSON'))
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_json, scope)
        drive_service = build('drive', 'v3', credentials=creds)
        print("âœ… êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì¸ì¦ ì„±ê³µ")
    except Exception as e:
        print(f"ğŸš¨ ì¸ì¦ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    # ì‚¬ìš©ìë‹˜ì˜ êµ¬ê¸€ ë“œë¼ì´ë¸Œ í´ë” ID
    FOLDER_ID = "1-aITCmfSiRZ1eNLnqvt071PyyqA9DjbT"
    
    # 2. í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    queries = ["Nasdaq", "S&P 500", "Nvidia", "FOMC", "Fed", "Inflation", "Trump", "Bitcoin", "Gold", "Oil"]
    all_data = []

    print(f"ğŸ“¡ ì´ {len(queries)}ê°œ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‹œì‘...")

    for q in queries:
        try:
            enc = urllib.parse.quote(q)
            # ìµœê·¼ 7ì¼ì¹˜ ë‰´ìŠ¤ ë°ì´í„° RSS ìš”ì²­
            url = f"https://news.google.com/rss/search?q={enc}+when:7d&hl=en-US&gl=US&ceid=US:en"
            feed = feedparser.parse(url)
            
            if feed.entries:
                print(f"âœ… {q}: {len(feed.entries)}ê°œ ìˆ˜ì§‘ ì„±ê³µ")
                for e in feed.entries:
                    all_data.append(f"{e.published} | {q} | {e.title}")
            else:
                print(f"âš ï¸ {q}: ë°ì´í„° ì—†ìŒ")
            time.sleep(0.5) # ì°¨ë‹¨ ë°©ì§€ìš© ë”œë ˆì´
        except Exception as e:
            print(f"âŒ {q} ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")

    if not all_data:
        print("ğŸš¨ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ìµœì¢… 0ê±´ì…ë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 3. ë°ì´í„° ë¶„í•  ë° ì—…ë¡œë“œ (ìš©ëŸ‰ ë¬¸ì œ í•´ê²° ì˜µì…˜ í¬í•¨)
    print(f"ğŸ“¦ ì´ {len(all_data)}ê°œ í—¤ë“œë¼ì¸ ì—…ë¡œë“œ ì‹œì‘...")
    chunk_size = 150
    
    for i in range(0, len(all_data), chunk_size):
        chunk = all_data[i:i + chunk_size]
        content = "DATE | CATEGORY | TITLE\n" + "="*50 + "\n" + "\n".join(chunk)
        
        file_name = f"MEGA_Archive_Part_{ (i//chunk_size)+1 :02d}.txt"
        meta = {'name': file_name, 'parents': [FOLDER_ID]}
        media = MediaInMemoryUpload(content.encode('utf-8'), mimetype='text/plain')
        
        try:
            # supportsAllDrives=True ì˜µì…˜ìœ¼ë¡œ ì„œë¹„ìŠ¤ ê³„ì •ì˜ ìš©ëŸ‰ ì œí•œ ìš°íšŒ
            drive_service.files().create(
                body=meta, 
                media_body=media,
                supportsAllDrives=True 
            ).execute()
            print(f"ğŸ“¤ {file_name} ì—…ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ {file_name} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

    print("--- âœ¨ ëª¨ë“  ì‘ì—…ì´ ì™„ë²½í•˜ê²Œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ ---")

if __name__ == "__main__":
    main()
