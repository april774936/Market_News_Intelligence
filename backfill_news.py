import os, json, urllib.parse, feedparser, time
from datetime import datetime
from googleapiclient.discovery import build
from googleapiclient.http import MediaInMemoryUpload
from oauth2client.service_account import ServiceAccountCredentials

def run_mega_backfill():
    # 1. ì¸ì¦ ë° ë“œë¼ì´ë¸Œ ì„¤ì •
    scope = ["https://www.googleapis.com/auth/drive.file"]
    creds_json = json.loads(os.environ.get('GSPREAD_JSON'))
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_json, scope)
    drive_service = build('drive', 'v3', credentials=creds)

    BACKFILL_FOLDER_ID = "1-aITCmfSiRZ1eNLnqvt071PyyqA9DjbT"

    # 2. ë°©ëŒ€í•œ ìˆ˜ì§‘ì„ ìœ„í•œ ì´ˆì •ë°€ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (50ê°œ í…Œë§ˆ)
    # ì§€ìˆ˜, ì„¹í„°, ì›ìì¬, ì •ì¹˜, ê±°ì‹œê²½ì œ ì§€í‘œ ë§ë¼
    search_queries = [
        "Nasdaq 100", "S&P 500", "Dow Jones", "Russell 2000", "VIX Index",
        "NVDA stock", "Apple AAPL", "Tesla TSLA", "Microsoft MSFT", "Google Alphabet",
        "Amazon AMZN", "Meta Platforms", "AMD semiconductor", "Broadcom AVGO", "ASML news",
        "Federal Reserve Interest Rate", "FOMC Meeting", "Jerome Powell Speech", "US Treasury Yield",
        "Inflation CPI", "PCE Price Index", "Unemployment rate", "GDP Growth", "Consumer Spending",
        "Trump Trade Policy", "US China Trade War", "Government Budget Deficit", "Tax Reform",
        "Gold price", "Silver price", "Crude Oil WTI", "Natural Gas", "Copper futures",
        "Bitcoin BTC", "Ethereum ETH", "Crypto regulation", "Coinbase news",
        "Dollar Index DXY", "EUR USD forex", "USD JPY yen", "USD KRW won",
        "JPMorgan Chase", "Goldman Sachs", "Bank of America", "Private Equity", "Hedge Fund",
        "AI revolution", "Cloud Computing", "Electric Vehicles", "Energy transition"
    ]
    
    all_headlines = []

    print(f"ğŸš€ ì´ {len(search_queries)}ê°œ í…Œë§ˆì— ëŒ€í•´ ì´ˆê±°ëŒ€ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    for q in search_queries:
        try:
            # ìµœëŒ€í•œ ë§ì€ ê³¼ê±°ì¹˜ë¥¼ ìœ„í•´ 'when:30d' (ì§€ë‚œ 30ì¼ê°„)ë¡œ í™•ì¥
            encoded_query = urllib.parse.quote(q)
            url = f"https://news.google.com/rss/search?q={encoded_query}+when:30d&hl=en-US&gl=US&ceid=US:en"
            feed = feedparser.parse(url)
            
            if not feed.entries:
                continue
                
            print(f"âœ… {q}: {len(feed.entries)}ê°œ ìˆ˜ì§‘")
            for entry in feed.entries:
                # ë°ì´í„°ë² ì´ìŠ¤ í˜•ì‹: [ë‚ ì§œ] | [í‚¤ì›Œë“œ] | [ì œëª©]
                all_headlines.append(f"{entry.published} | {q} | {entry.title}")
            
            # êµ¬ê¸€ ì„œë²„ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ìµœì†Œí•œì˜ ë”œë ˆì´
            time.sleep(0.5)
        except Exception as e:
            print(f"âŒ {q} ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")

    if not all_headlines:
        print("ğŸš¨ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì¤‘ë³µ ì œê±° (ì—¬ëŸ¬ í‚¤ì›Œë“œì— ê²¹ì¹˜ëŠ” ë‰´ìŠ¤ ì œê±°)
    all_headlines = list(set(all_headlines))
    all_headlines.sort() # ì‹œê°„ìˆœ ì •ë ¬
    print(f"ğŸ”¥ ì¤‘ë³µ ì œê±° í›„ ì´ {len(all_headlines)}ê°œì˜ í—¤ë“œë¼ì¸ í™•ë³´!")

    # 3. ë°ì´í„° ë¶„í•  ì €ì¥ (NotebookLM ê°œë³„ íŒŒì¼ ìš©ëŸ‰ ìµœì í™”)
    # íŒŒì¼ë‹¹ 200ì¤„ì”© ì €ì¥í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ íŒŒì¼ë¡œ ìƒì„±
    chunk_size = 200 
    for i in range(0, len(all_headlines), chunk_size):
        chunk = all_headlines[i:i + chunk_size]
        file_content = "DATE | CATEGORY | HEADLINE\n" + "="*50 + "\n"
        file_content += "\n".join(chunk)
        
        part_num = (i // chunk_size) + 1
        file_name = f"MEGA_Archive_Part_{part_num:02d}.txt"

        file_metadata = {'name': file_name, 'parents': [BACKFILL_FOLDER_ID]}
        media = MediaInMemoryUpload(file_content.encode('utf-8'), mimetype='text/plain')
        
        try:
            drive_service.files().create(body=file_metadata, media_body=media).execute()
            print(f"ğŸ“¤ {file_name} ì—…ë¡œë“œ ì™„ë£Œ (ë¼ì¸ {i}~{i+len(chunk)})")
        except: continue

    print(f"âœ¨ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. êµ¬ê¸€ ë“œë¼ì´ë¸Œ í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”!")

if __name__ == "__main__":
    run_mega_backfill()
