import yfinance as yf
import os, json
from datetime import datetime
from googleapiclient.discovery import build
from googleapiclient.http import MediaInMemoryUpload
from oauth2client.service_account import ServiceAccountCredentials

def run_backfill():
    # 1. ì¸ì¦ ë° ë“œë¼ì´ë¸Œ ì„¤ì •
    scope = ["https://www.googleapis.com/auth/drive.file"]
    creds_json = json.loads(os.environ.get('GSPREAD_JSON'))
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_json, scope)
    drive_service = build('drive', 'v3', credentials=creds)

    BACKFILL_FOLDER_ID = "1-aITCmfSiRZ1eNLnqvt071PyyqA9DjbT"

    # 2. ìˆ˜ì§‘ ëŒ€ìƒ í™•ì¥ (ì§€ìˆ˜ëŠ” ë‰´ìŠ¤ê°€ ì ì„ ìˆ˜ ìˆì–´ ê´€ë ¨ ëŒ€í˜•ì£¼ ëŒ€ê±° ì¶”ê°€)
    # ë‚˜ìŠ¤ë‹¥/S&P500ì„ ëŒ€ë³€í•˜ëŠ” í•µì‹¬ ì¢…ëª©ë“¤
    tickers = [
        "QQQ", "SPY", "DIA", "NVDA", "AAPL", "TSLA", "MSFT", "GOOGL", "AMZN", "META",
        "TQQQ", "SOXL", "AMD", "ASML", "JPM", "GS", "BRK-B"
    ]
    
    all_headlines = []

    print(f"ğŸ” {len(tickers)}ê°œ ì¢…ëª©ì—ì„œ ê³¼ê±° ë‰´ìŠ¤ ì¶”ì¶œ ì‹œì‘...")
    
    for t in tickers:
        try:
            ticker_obj = yf.Ticker(t)
            # yfinanceëŠ” ìµœì‹  ë‰´ìŠ¤ 10~20ê°œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
            news_list = ticker_obj.news
            if not news_list:
                print(f"âš ï¸ {t}: ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
                
            print(f"âœ… {t}: {len(news_list)}ê°œ ë°œê²¬")
            for n in news_list:
                # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
                dt = datetime.fromtimestamp(n['providerPublishTime']).strftime('%Y-%m-%d %H:%M')
                title = n['title']
                publisher = n.get('publisher', 'Unknown')
                # ë°ì´í„°ë² ì´ìŠ¤ í˜•ì‹: [ë‚ ì§œ] | [ì¢…ëª©] | [ì¶œì²˜] | [í—¤ë“œë¼ì¸]
                all_headlines.append(f"{dt} | {t} | {publisher} | {title}")
        except Exception as e:
            print(f"âŒ {t} ì—ëŸ¬: {e}")
            continue

    if not all_headlines:
        print("ğŸš¨ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤. í‹°ì»¤ë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return

    # 3. ë°ì´í„° ì •ë ¬ ë° ë¶„í•  ì €ì¥
    all_headlines.sort() # ë‚ ì§œìˆœ ì •ë ¬
    print(f"ì´ {len(all_headlines)}ê°œì˜ í—¤ë“œë¼ì¸ì„ ì •ë¦¬í•©ë‹ˆë‹¤.")
    
    # NotebookLM í•™ìŠµì„ ìœ„í•´ íŒŒì¼ë‹¹ 100ì¤„ì”© ë¶„í• 
    chunk_size = 100 
    for i in range(0, len(all_headlines), chunk_size):
        chunk = all_headlines[i:i + chunk_size]
        file_content = "\n".join(chunk)
        part_num = (i // chunk_size) + 1
        file_name = f"Historical_News_DB_Part_{part_num:02d}.txt"

        file_metadata = {'name': file_name, 'parents': [BACKFILL_FOLDER_ID]}
        media = MediaInMemoryUpload(file_content.encode('utf-8'), mimetype='text/plain')
        
        try:
            drive_service.files().create(body=file_metadata, media_body=media).execute()
            print(f"ğŸ“¤ {file_name} ì—…ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"ğŸš¨ {file_name} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    run_backfill()
